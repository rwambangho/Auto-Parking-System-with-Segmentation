{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 미리 학습된 모델에 이미지와 동영상을 넣어 output 만들기"],"metadata":{"id":"RbtsDRf2Hc-M"}},{"cell_type":"markdown","source":["### Step 1. 모델 정의"],"metadata":{"id":"ef2Yt-TN2vrB"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class UNet(nn.Module):\n","    def __init__(self):\n","        super(UNet, self).__init__()\n","\n","        def CBR(in_channels, out_channels):\n","            layers = [\n","                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(out_channels),\n","                nn.ReLU(inplace=True)\n","            ]\n","            return nn.Sequential(*layers)\n","\n","        # Contracting path\n","        self.enc1 = nn.Sequential(CBR(1, 64), CBR(64, 64))\n","        self.pool1 = nn.MaxPool2d(2)\n","\n","        self.enc2 = nn.Sequential(CBR(64, 128), CBR(128, 128))\n","        self.pool2 = nn.MaxPool2d(2)\n","\n","        self.enc3 = nn.Sequential(CBR(128, 256), CBR(256, 256))\n","        self.pool3 = nn.MaxPool2d(2)\n","\n","        self.enc4 = nn.Sequential(CBR(256, 512), CBR(512, 512))\n","        self.pool4 = nn.MaxPool2d(2)\n","\n","        self.center = CBR(512, 1024)\n","\n","        # Expansive path\n","        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n","        self.dec4 = nn.Sequential(CBR(1024, 512), CBR(512, 512))\n","\n","        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n","        self.dec3 = nn.Sequential(CBR(512, 256), CBR(256, 256))\n","\n","        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n","        self.dec2 = nn.Sequential(CBR(256, 128), CBR(128, 128))\n","\n","        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n","        self.dec1 = nn.Sequential(CBR(128, 64), CBR(64, 64))\n","\n","        #self.final = nn.Conv2d(64, 1, kernel_size=1)\n","        #클래스 수 변경하기 epochs5 이후에 음수 validation loss 값이 나와서 변경\n","        self.final = nn.Conv2d(64, 3, kernel_size=1)  # 3은 클래스 수\n","\n","    def forward(self, x):\n","      # Encoder\n","      enc1 = self.enc1(x)\n","      pool1 = self.pool1(enc1)\n","\n","      enc2 = self.enc2(pool1)\n","      pool2 = self.pool2(enc2)\n","\n","      enc3 = self.enc3(pool2)\n","      pool3 = self.pool3(enc3)\n","\n","      enc4 = self.enc4(pool3)\n","      pool4 = self.pool4(enc4)\n","\n","      # Center\n","      center = self.center(pool4)\n","\n","      # Decoder\n","      up4 = self.up4(center)\n","\n","      # 크기를 맞추기 위해 자르기 (업샘플링 후 텐서 크기 불일치 문제 해결)\n","      if up4.size() != enc4.size():\n","          diffY = enc4.size(2) - up4.size(2)\n","          diffX = enc4.size(3) - up4.size(3)\n","          up4 = torch.nn.functional.pad(up4, [0, diffX, 0, diffY])\n","\n","      merge4 = torch.cat([up4, enc4], dim=1)\n","      dec4 = self.dec4(merge4)\n","\n","      up3 = self.up3(dec4)\n","      if up3.size() != enc3.size():\n","          diffY = enc3.size(2) - up3.size(2)\n","          diffX = enc3.size(3) - up3.size(3)\n","          up3 = torch.nn.functional.pad(up3, [0, diffX, 0, diffY])\n","\n","      merge3 = torch.cat([up3, enc3], dim=1)\n","      dec3 = self.dec3(merge3)\n","\n","      up2 = self.up2(dec3)\n","      if up2.size() != enc2.size():\n","          diffY = enc2.size(2) - up2.size(2)\n","          diffX = enc2.size(3) - up2.size(3)\n","          up2 = torch.nn.functional.pad(up2, [0, diffX, 0, diffY])\n","\n","      merge2 = torch.cat([up2, enc2], dim=1)\n","      dec2 = self.dec2(merge2)\n","\n","      up1 = self.up1(dec2)\n","      if up1.size() != enc1.size():\n","          diffY = enc1.size(2) - up1.size(2)\n","          diffX = enc1.size(3) - up1.size(3)\n","          up1 = torch.nn.functional.pad(up1, [0, diffX, 0, diffY])\n","\n","      merge1 = torch.cat([up1, enc1], dim=1)\n","      dec1 = self.dec1(merge1)\n","\n","      out = self.final(dec1)\n","      return out"],"metadata":{"id":"r4kDsdqS9Pnw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 2. 데이터셋 클래스 및 Transform 정의\n","\n","PyTorch의 Dataset과 DataLoader를 사용하여 데이터를 불러옵니다."],"metadata":{"id":"wjwvHhGn38PH"}},{"cell_type":"code","source":["import os\n","import json\n","import numpy as np\n","import cv2\n","import torch\n","from torch.utils.data import Dataset\n","\n","class PS_Dataset(Dataset):\n","    def __init__(self, data_dir, transform=None):\n","        self.data_dir = data_dir\n","        self.transform = transform\n","        self.input_paths = sorted([os.path.join(data_dir, 'images', f) for f in os.listdir(os.path.join(data_dir, 'images')) if f.endswith('.jpg')])\n","        self.label_paths = sorted([os.path.join(data_dir, 'labels', f) for f in os.listdir(os.path.join(data_dir, 'labels')) if f.endswith('.json')])\n","\n","    def __len__(self):\n","        return len(self.input_paths)\n","\n","    def __getitem__(self, idx):\n","        # 이미지 로드 (jpg 파일)\n","        input_image = cv2.imread(self.input_paths[idx], cv2.IMREAD_GRAYSCALE)  # 흑백으로 읽음\n","        input_image = input_image / 255.0  # 정규화 [0, 1]\n","        input_image = np.expand_dims(input_image, axis=0).astype(np.float32)  # 채널 차원 추가 (C, H, W)\n","\n","        # 라벨 로드 (json 파일)\n","        with open(self.label_paths[idx], 'r') as f:\n","            label_data = json.load(f)\n","\n","        # JSON 데이터를 기반으로 다중 클래스 레이블 생성\n","        label_image = np.zeros((input_image.shape[1], input_image.shape[2]), dtype=np.uint8)  # (H, W) 크기\n","\n","        for region in label_data['segmentation']:\n","            label_name = region['name']  # 클래스 이름: \"Driveable Space\" 또는 \"Parking Space\"\n","            points = np.array(region['polygon'], dtype=np.int32)  # 폴리곤 좌표\n","\n","            # 클래스 할당 (0: Background, 1: Driveable Space, 2: Parking Space)\n","            if label_name == 'Driveable Space':\n","                cv2.fillPoly(label_image, [points], 1)  # Driveable Space -> 클래스 1\n","            elif label_name == 'Parking Space':\n","                cv2.fillPoly(label_image, [points], 2)  # Parking Space -> 클래스 2\n","\n","        label_image = label_image.astype(np.float32)  # Float 변환\n","        label_image = np.expand_dims(label_image, axis=0)  # (C, H, W) 차원 추가\n","\n","        # Tensor로 변환\n","        input_image = torch.from_numpy(input_image)\n","        label_image = torch.from_numpy(label_image)\n","\n","        # Transform 적용 (필요한 경우)\n","        if self.transform:\n","            input_image = self.transform(input_image)\n","\n","        return {'input': input_image, 'label': label_image}"],"metadata":{"id":"qz4IONmEKaYd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3.1 Transform 정의"],"metadata":{"id":"mDcMWNVP4BVp"}},{"cell_type":"code","source":["import torchvision.transforms as transforms\n","\n","transform = transforms.Compose([\n","    transforms.Normalize(mean=[0.5], std=[0.5])\n","])"],"metadata":{"id":"cQSyO-6b4DZi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### 학습된 모델을 불러와서 성능 검증하기"],"metadata":{"id":"mZUDcP9cCL1f"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TY2prOicCMu9","executionInfo":{"status":"ok","timestamp":1732032977990,"user_tz":-540,"elapsed":2686,"user":{"displayName":"이재희","userId":"06733004630501728188"}},"outputId":"887e8eb5-4a34-4187-859c-a5df10f9fd74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["- 동영상을 넣어서 성능 검증하기"],"metadata":{"id":"P87XuTk8D83d"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import torch\n","\n","# 모델 경로\n","model_path = '/content/drive/MyDrive/likelion_CV/segmentation_project/unet_model50.pth'\n","\n","# 동영상 파일 경로\n","video_path = '/content/drive/MyDrive/likelion_CV/segmentation_project/parkingspace_video(640,360).mp4'\n","\n","# 결과 저장 경로\n","output_video_path = '/content/drive/MyDrive/likelion_CV/segmentation_project/parkingspace_video_output50.mp4'\n","\n","# 색상 맵핑 설정\n","segmentation_colors = {\n","    0: (0, 0, 0),       # Background\n","    1: (0, 255, 0),     # Driveable Space\n","    2: (255, 0, 0)      # Parking Space\n","}\n","\n","# GPU/CPU 설정\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# 모델 초기화 및 가중치 불러오기\n","model = UNet()  # UNet 클래스 정의 필요\n","model.load_state_dict(torch.load(model_path, map_location=device))  # 가중치 로드\n","model.eval()  # 평가 모드 설정\n","model.to(device)  # 모델을 GPU/CPU로 이동\n","\n","# 동영상 로드\n","cap = cv2.VideoCapture(video_path)\n","\n","# 동영상 정보 가져오기\n","fps = int(cap.get(cv2.CAP_PROP_FPS))\n","width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","# 출력 동영상 코덱 설정 (MP4V 코덱 사용)\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n","\n","# 프레임 단위로 동영상 처리\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # 프레임 전처리 (모델 입력 크기에 맞게 리사이즈 및 그레이스케일 변환)\n","    input_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # 컬러 이미지를 그레이스케일로 변환\n","    input_frame_resized = cv2.resize(input_frame, (640, 360))  # 모델 크기에 맞게 리사이즈\n","    input_tensor = torch.from_numpy(input_frame_resized).unsqueeze(0).unsqueeze(0).float() / 255.0  # 1채널로 변경\n","    input_tensor = input_tensor.to(device)\n","\n","    # 모델 예측\n","    with torch.no_grad():\n","        outputs = model(input_tensor)\n","        outputs = torch.softmax(outputs, dim=1)\n","        predicted_classes = torch.argmax(outputs, dim=1).cpu().numpy().squeeze()\n","\n","    # 세그멘테이션 결과를 컬러 이미지로 변환\n","    output_colored = np.zeros((*predicted_classes.shape, 3), dtype=np.uint8)\n","    for class_id, color in segmentation_colors.items():\n","        output_colored[predicted_classes == class_id] = color\n","\n","    # 결과 이미지 원본 크기로 리사이즈\n","    output_colored = cv2.resize(output_colored, (width, height))\n","\n","    # 결과 이미지와 원본 이미지 합성\n","    overlay = cv2.addWeighted(frame, 0.6, output_colored, 0.4, 0)\n","\n","    # 결과 동영상 저장\n","    out.write(overlay)\n","\n","# 모든 작업 완료 후 자원 해제\n","cap.release()\n","out.release()\n","print(f\"처리가 완료되었습니다. 결과 동영상은 {output_video_path}에 저장되었습니다.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lF-GcGv4I2Q6","executionInfo":{"status":"ok","timestamp":1732027616175,"user_tz":-540,"elapsed":41248,"user":{"displayName":"이재희","userId":"06733004630501728188"}},"outputId":"056e8ced-7009-4b14-e1b0-3e4baee05538"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-bccebb679c47>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_path, map_location=device))  # 가중치 로드\n"]},{"output_type":"stream","name":"stdout","text":["처리가 완료되었습니다. 결과 동영상은 /content/drive/MyDrive/likelion_CV/segmentation_project/parkingspace_video_output50.mp4에 저장되었습니다.\n"]}]},{"cell_type":"markdown","source":["- 이미지를 세그멘테이션으로 만든 후에 동영상으로 변환"],"metadata":{"id":"krThvUMG3jFS"}},{"cell_type":"code","source":["import os\n","import torch\n","import cv2\n","import numpy as np\n","from torch import nn\n","\n","# 모델 경로\n","model_path = '/content/drive/MyDrive/likelion_CV/segmentation_project/unet_model50.pth'\n","\n","# 출력 결과를 저장할 폴더\n","output_folder = '/content/drive/MyDrive/likelion_CV/segmentation_project/segmentation_results2'\n","\n","# 출력 폴더가 존재하지 않으면 생성\n","if not os.path.exists(output_folder):\n","    os.makedirs(output_folder)\n","\n","\n","# 모델 초기화 및 가중치 불러오기\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = UNet()\n","model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n","model.to(device)\n","model.eval()\n","\n","# 색상 맵핑 설정\n","segmentation_colors = {\n","    0: (0, 0, 0),       # Background (검정색)\n","    1: (0, 255, 0),     # Driveable Space (녹색)\n","    2: (255, 0, 0)      # Parking Space (빨간색)\n","}\n","\n","# 입력 이미지 폴더 경로\n","image_folder = '/content/drive/MyDrive/likelion_CV/segmentation_project/im2'\n","\n","# 폴더 내 모든 이미지 처리\n","for image_name in os.listdir(image_folder):\n","    image_path = os.path.join(image_folder, image_name)\n","\n","    # 이미지 로드 및 전처리\n","    image = cv2.imread(image_path)\n","    if image is None:\n","        print(f\"이미지를 불러올 수 없습니다: {image_name}\")\n","        continue\n","\n","    original_height, original_width = image.shape[:2]\n","    image_resized = cv2.resize(image, (640, 360))  # 모델 입력 크기에 맞게 리사이즈\n","    input_frame = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY)  # 그레이스케일 변환\n","    input_tensor = torch.from_numpy(input_frame).unsqueeze(0).unsqueeze(0).float().to(device) / 255.0  # 정규화 및 차원 추가\n","\n","    # 모델 예측\n","    with torch.no_grad():\n","        output = model(input_tensor)\n","        predicted_classes = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n","\n","    # 세그멘테이션 결과를 컬러로 변환\n","    output_colored = np.zeros((*predicted_classes.shape, 3), dtype=np.uint8)\n","    for class_id, color in segmentation_colors.items():\n","        output_colored[predicted_classes == class_id] = color\n","\n","    # 원본 이미지 크기로 리사이즈\n","    output_colored = cv2.resize(output_colored, (original_width, original_height))\n","\n","    # 원본 이미지와 세그멘테이션 결과를 오버레이\n","    overlay = cv2.addWeighted(image, 0.6, output_colored, 0.4, 0)\n","\n","    # 결과 저장\n","    output_image_path = os.path.join(output_folder, f\"overlay_{image_name}\")\n","    cv2.imwrite(output_image_path, overlay)\n","    #print(f\"오버레이 세그멘테이션 결과 저장: {output_image_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Ck1rFhBPErv","executionInfo":{"status":"ok","timestamp":1732034416186,"user_tz":-540,"elapsed":26283,"user":{"displayName":"이재희","userId":"06733004630501728188"}},"outputId":"9decfd2f-61f3-4d42-c9be-6910b0ff11e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-24-99bdde366b27>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","\n","# 세그멘테이션 결과 이미지 폴더 경로\n","segmentation_folder = '/content/drive/MyDrive/likelion_CV/segmentation_project/segmentation_results2'\n","\n","# 출력 동영상 경로\n","output_video_path = '/content/drive/MyDrive/likelion_CV/segmentation_project/segmentation_imtovi_output.mp4'\n","\n","# 이미지 파일 리스트 정렬\n","image_files = sorted(os.listdir(segmentation_folder))\n","\n","# 동영상 저장 설정\n","fps = 10  # 초당 프레임 수 (FPS)\n","frame_size = None  # 프레임 크기 (첫 번째 이미지에서 가져올 예정)\n","\n","# VideoWriter 초기화\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # MP4V 코덱\n","video_writer = None\n","\n","# 이미지 순서대로 읽어서 동영상 생성\n","for image_file in image_files:\n","    image_path = os.path.join(segmentation_folder, image_file)\n","    frame = cv2.imread(image_path)\n","\n","    if frame is None:\n","        print(f\"이미지를 불러올 수 없습니다: {image_file}\")\n","        continue\n","\n","    if frame_size is None:\n","        # 첫 번째 프레임 크기로 설정\n","        frame_size = (frame.shape[1], frame.shape[0])  # (width, height)\n","        video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n","\n","    video_writer.write(frame)\n","\n","# VideoWriter 자원 해제\n","if video_writer is not None:\n","    video_writer.release()\n","\n","print(f\"영상 생성 완료: {output_video_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gg4NCAfyS2Vc","executionInfo":{"status":"ok","timestamp":1732034650311,"user_tz":-540,"elapsed":6803,"user":{"displayName":"이재희","userId":"06733004630501728188"}},"outputId":"5b9fc834-0a31-4ee1-b388-7a0f2e7da4f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["영상 생성 완료: /content/drive/MyDrive/likelion_CV/segmentation_project/segmentation_video_output.mp4\n"]}]}]}